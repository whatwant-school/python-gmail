"""
Blog utility functions for searching latest blog posts by keyword
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import requests
from bs4 import BeautifulSoup


def search_blogs_by_keyword(
    keyword: str, max_results: int = 5, hours_back: int = 24
) -> List[Dict[str, Any]]:
    """
    í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœê·¼ ë¸”ë¡œê·¸ ê¸€ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜ (3-5ê°œ ê¶Œì¥)
        hours_back (int): ê²€ìƒ‰í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)

    Returns:
        List[Dict[str, Any]]: ë¸”ë¡œê·¸ ê¸€ ëª©ë¡
    """
    try:
        # Google Blog Search RSSë¥¼ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê²€ìƒ‰
        import urllib.parse

        search_query = f"{keyword} -ê´‘ê³  -í™ë³´"  # ê´‘ê³ , í™ë³´ ì œì™¸
        encoded_query = urllib.parse.quote(search_query)

        # Google Blogs RSS ê²€ìƒ‰ (í‹°ìŠ¤í† ë¦¬, ë„¤ì´ë²„ ë¸”ë¡œê·¸, ë¸ŒëŸ°ì¹˜)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}+when:1d+site:tistory.com+OR+site:blog.naver.com+OR+site:brunch.co.kr&hl=ko&gl=KR&ceid=KR:ko"

        # RSS2JSON API ì‚¬ìš© (ë¬´ë£Œ ì„œë¹„ìŠ¤)
        encoded_rss_url = urllib.parse.quote(rss_url, safe="")
        api_url = f"https://api.rss2json.com/v1/api.json?rss_url={encoded_rss_url}"

        response = requests.get(api_url, timeout=10)

        if response.status_code != 200:
            return _fallback_blog_search(keyword, max_results)

        data = response.json()

        if data.get("status") != "ok":
            return _fallback_blog_search(keyword, max_results)

        articles = data.get("items", [])

        # ì‹œê°„ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        filtered_blogs = []
        seen_titles = set()

        for article in articles:
            # ì œëª© ì¤‘ë³µ ì²´í¬ (ìœ ì‚¬í•œ ê¸€ ì œì™¸)
            title = article.get("title", "")
            if not title or _is_similar_title(title, seen_titles):
                continue

            # ê´‘ê³ /í™ë³´ ê¸€ í•„í„°ë§
            if _is_ad_or_promotional(title, article.get("description", "")):
                continue

            # ë°œí–‰ ì‹œê°„ íŒŒì‹±
            pub_date = _parse_pub_date(article.get("pubDate", ""))
            if pub_date and pub_date < cutoff_time:
                continue

            # ë¸”ë¡œê·¸ ì •ë³´ êµ¬ì„±
            description = article.get("description", "")
            link = article.get("link", "")

            # ë¸”ë¡œê·¸ ì¶œì²˜ ì¶”ì¶œ
            source = _extract_blog_source(link, title, description)

            # ë¸”ë¡œê·¸ ë³¸ë¬¸ì„ ê°€ì ¸ì™€ì„œ ê°œì„ ëœ ìš”ì•½ ìƒì„±
            try:
                blog_content = _fetch_blog_content(link) if link else ""
                summary = _generate_summary(title, blog_content, description)
            except Exception:
                summary = _clean_description(description)

            blog_item = {
                "title": title,
                "summary": summary,
                "source": source,
                "link": link,
                "pub_date": pub_date.strftime("%Y-%m-%d %H:%M")
                if pub_date
                else "ì‹œê°„ ì •ë³´ ì—†ìŒ",
                "pub_date_raw": pub_date,
            }

            filtered_blogs.append(blog_item)
            seen_titles.add(_normalize_title(title))

            if len(filtered_blogs) >= max_results:
                break

        # ë°œí–‰ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        filtered_blogs.sort(
            key=lambda x: x.get("pub_date_raw") or datetime.min, reverse=True
        )

        # ìµœì¢… ê²°ê³¼ì—ì„œ raw ì‹œê°„ ì •ë³´ ì œê±°
        for blog in filtered_blogs:
            blog.pop("pub_date_raw", None)

        return filtered_blogs[:max_results]

    except Exception as e:
        print(f"ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return _fallback_blog_search(keyword, max_results)


def _fallback_blog_search(keyword: str, max_results: int = 5) -> List[Dict[str, Any]]:
    """
    ì£¼ APIê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ ì‚¬ìš©í•  ëŒ€ì²´ ë¸”ë¡œê·¸ ê²€ìƒ‰ ë°©ë²•

    Args:
        keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        List[Dict[str, Any]]: ë¸”ë¡œê·¸ ê¸€ ëª©ë¡
    """
    # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš°
    return [
        {
            "title": f"{keyword} ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹¤íŒ¨",
            "summary": "ë„¤íŠ¸ì›Œí¬ ë¬¸ì œë‚˜ API ì„œë¹„ìŠ¤ ì¥ì• ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "source": "ì‹œìŠ¤í…œ ì˜¤ë¥˜",
            "link": "",
            "pub_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
        }
    ]


def _parse_pub_date(date_str: str) -> Optional[datetime]:
    """
    ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜

    Args:
        date_str (str): ë‚ ì§œ ë¬¸ìì—´

    Returns:
        Optional[datetime]: íŒŒì‹±ëœ datetime ê°ì²´ ë˜ëŠ” None
    """
    if not date_str:
        return None

    # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
    formats = [
        "%a, %d %b %Y %H:%M:%S %Z",  # RSS í‘œì¤€ í˜•ì‹
        "%a, %d %b %Y %H:%M:%S %z",  # RFC 2822
        "%Y-%m-%dT%H:%M:%S%z",  # ISO 8601
        "%Y-%m-%d %H:%M:%S",  # ì¼ë°˜ì ì¸ í˜•ì‹
        "%Y-%m-%d",  # ë‚ ì§œë§Œ
    ]

    for fmt in formats:
        try:
            return datetime.strptime(date_str.strip(), fmt)
        except ValueError:
            continue

    # í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ë„ ì‹œë„
    try:
        # "2024ë…„ 1ì›” 15ì¼" í˜•ì‹ ì²˜ë¦¬
        import re

        korean_date = re.search(r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼", date_str)
        if korean_date:
            year, month, day = korean_date.groups()
            return datetime(int(year), int(month), int(day))
    except Exception:
        pass

    return None


def _is_similar_title(title: str, seen_titles: set) -> bool:
    """
    ì œëª©ì´ ì´ë¯¸ ë³¸ ì œëª©ë“¤ê³¼ ìœ ì‚¬í•œì§€ í™•ì¸

    Args:
        title (str): í™•ì¸í•  ì œëª©
        seen_titles (set): ì´ë¯¸ ë³¸ ì œëª©ë“¤ì˜ ì§‘í•©

    Returns:
        bool: ìœ ì‚¬í•œ ì œëª©ì´ ìˆìœ¼ë©´ True
    """
    normalized_title = _normalize_title(title)

    for seen_title in seen_titles:
        # 50% ì´ìƒ ê²¹ì¹˜ë©´ ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ íŒë‹¨
        if _calculate_similarity(normalized_title, seen_title) > 0.5:
            return True

    return False


def _normalize_title(title: str) -> str:
    """
    ì œëª©ì„ ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)

    Args:
        title (str): ì›ë³¸ ì œëª©

    Returns:
        str: ì •ê·œí™”ëœ ì œëª©
    """
    import re

    # íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜
    normalized = re.sub(r"[^\w\sê°€-í£]", "", title)
    normalized = re.sub(r"\s+", " ", normalized).strip().lower()
    return normalized


def _calculate_similarity(str1: str, str2: str) -> float:
    """
    ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)

    Args:
        str1 (str): ì²« ë²ˆì§¸ ë¬¸ìì—´
        str2 (str): ë‘ ë²ˆì§¸ ë¬¸ìì—´

    Returns:
        float: ìœ ì‚¬ë„ (0.0 ~ 1.0)
    """
    words1 = set(str1.split())
    words2 = set(str2.split())

    if not words1 and not words2:
        return 1.0

    intersection = words1.intersection(words2)
    union = words1.union(words2)

    return len(intersection) / len(union) if union else 0.0


def _is_ad_or_promotional(title: str, description: str) -> bool:
    """
    ê´‘ê³ ë‚˜ í™ë³´ì„± ê¸€ì¸ì§€ í™•ì¸

    Args:
        title (str): ê¸€ ì œëª©
        description (str): ê¸€ ì„¤ëª…

    Returns:
        bool: ê´‘ê³ /í™ë³´ ê¸€ì´ë©´ True
    """
    content = (title + " " + description).lower()

    # ê´‘ê³ /í™ë³´ ê´€ë ¨ í‚¤ì›Œë“œ
    ad_keywords = [
        "ê´‘ê³ ",
        "í™ë³´",
        "í˜‘ì°¬",
        "ì œíœ´",
        "í• ì¸",
        "ì´ë²¤íŠ¸",
        "í”„ë¡œëª¨ì…˜",
        "ë§ˆì¼€íŒ…",
        "ë¸Œëœë“œ",
        "ë¡ ì¹­",
        "ì˜¤í”ˆ",
        "ì‹ ì œí’ˆ",
        "ì¶œì‹œ",
        "íŠ¹ê°€",
        "ì„¸ì¼",
        "ì¿ í°",
        "í¬ì¸íŠ¸",
        "í˜œíƒ",
        "ë¬´ë£Œì²´í—˜",
        "[ê´‘ê³ ]",
        "(ê´‘ê³ )",
        "[pr]",
        "(pr)",
        "[í™ë³´]",
        "(í™ë³´)",
    ]

    return any(keyword in content for keyword in ad_keywords)


def _clean_description(description: str) -> str:
    """
    ê¸€ ì„¤ëª…ì—ì„œ HTML íƒœê·¸ë‚˜ ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°í•˜ê³  ì‹¤ì œ ë³¸ë¬¸ ìš”ì•½ ìƒì„±

    Args:
        description (str): ì›ë³¸ ì„¤ëª…

    Returns:
        str: ì •ì œëœ ì„¤ëª…
    """
    import re

    if not description:
        return "ìš”ì•½ ì •ë³´ ì—†ìŒ"

    # HTML íƒœê·¸ ì œê±°
    clean_desc = re.sub(r"<[^>]+>", "", description)

    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    clean_desc = re.sub(r"\s+", " ", clean_desc).strip()

    # ë¸”ë¡œê·¸ ì¶œì²˜ ì •ë³´ë‚˜ ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì œê±°
    clean_desc = re.sub(
        r"\s*-\s*[ê°€-í£A-Za-z0-9]+(?:ë¸”ë¡œê·¸|BLOG|Blog)?\s*$", "", clean_desc
    )

    # URLì´ë‚˜ ë„ë©”ì¸ ì •ë³´ ì œê±°
    clean_desc = re.sub(r"https?://[^\s]+", "", clean_desc)
    clean_desc = re.sub(r"www\.[^\s]+", "", clean_desc)
    clean_desc = re.sub(
        r"[a-zA-Z0-9.-]+\.(com|net|co\.kr|org|tistory\.com|naver\.com)\b",
        "",
        clean_desc,
    )

    # ë„ˆë¬´ ì§§ì€ ì„¤ëª…
    if len(clean_desc.strip()) < 20:
        return "ë¸”ë¡œê·¸ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”"

    # ì—°ì†ëœ ê³µë°± ë‹¤ì‹œ ì •ë¦¬
    clean_desc = re.sub(r"\s+", " ", clean_desc).strip()

    # ë„ˆë¬´ ê¸´ ì„¤ëª…ì€ ì˜ë¼ë‚´ê¸° (150ì ì œí•œ)
    if len(clean_desc) > 150:
        sentences = clean_desc.split(". ")
        if len(sentences) > 1:
            clean_desc = sentences[0] + "."
            if len(clean_desc) > 150:
                clean_desc = clean_desc[:147] + "..."
        else:
            clean_desc = clean_desc[:147] + "..."

    return (
        clean_desc if clean_desc.strip() else "ë¸”ë¡œê·¸ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”"
    )


def _extract_blog_source(link: str, title: str = "", description: str = "") -> str:
    """
    ë¸”ë¡œê·¸ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ

    Args:
        link (str): ë¸”ë¡œê·¸ ë§í¬
        title (str): ê¸€ ì œëª©
        description (str): ê¸€ ì„¤ëª…

    Returns:
        str: ì¶œì²˜ ì´ë¦„
    """
    import re
    from urllib.parse import urlparse

    # 1. ì œëª©ì—ì„œ ë¸”ë¡œê·¸ í”Œë«í¼ ì •ë³´ ì¶”ì¶œ (Google News RSSëŠ” ì œëª©ì— ì¶œì²˜ í¬í•¨)
    if title:
        # "ì œëª© : ë„¤ì´ë²„ ë¸”ë¡œê·¸" ë˜ëŠ” "ì œëª© - ë„¤ì´ë²„" íŒ¨í„´
        if "ë„¤ì´ë²„ ë¸”ë¡œê·¸" in title or ": ë„¤ì´ë²„ ë¸”ë¡œê·¸" in title:
            # "ì œëª© : ë„¤ì´ë²„ ë¸”ë¡œê·¸ - NAVER" í˜•íƒœì—ì„œ ì œëª© ë¶€ë¶„ë§Œ ì¶”ì¶œ
            title_parts = re.split(r'\s*[:ï¼š]\s*ë„¤ì´ë²„\s*ë¸”ë¡œê·¸', title)
            if title_parts and len(title_parts[0]) > 0:
                clean_title = title_parts[0].strip()
                return f"{clean_title[:30]}... (ë„¤ì´ë²„ ë¸”ë¡œê·¸)"
            return "ë„¤ì´ë²„ ë¸”ë¡œê·¸"

        # "ì œëª© - í‹°ìŠ¤í† ë¦¬" íŒ¨í„´
        if "í‹°ìŠ¤í† ë¦¬" in title or "- í‹°ìŠ¤í† ë¦¬" in title:
            title_parts = re.split(r'\s*-\s*í‹°ìŠ¤í† ë¦¬', title)
            if title_parts and len(title_parts[0]) > 0:
                clean_title = title_parts[0].strip()
                return f"{clean_title[:30]}... (í‹°ìŠ¤í† ë¦¬)"
            return "í‹°ìŠ¤í† ë¦¬"

        # "ì œëª© - ë¸ŒëŸ°ì¹˜" íŒ¨í„´
        if "ë¸ŒëŸ°ì¹˜" in title or "brunch" in title.lower():
            return "ë¸ŒëŸ°ì¹˜"

        # "NAVER" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë„¤ì´ë²„ ë¸”ë¡œê·¸ë¡œ ì¶”ì •
        if "NAVER" in title or "naver" in title.lower():
            return "ë„¤ì´ë²„ ë¸”ë¡œê·¸"

    # 2. ë§í¬ì—ì„œ ë¸”ë¡œê·¸ í”Œë«í¼ ë° ì‚¬ìš©ì ì¶”ì¶œ
    if link:
        try:
            parsed_url = urlparse(link)
            domain = parsed_url.netloc.lower()

            # í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸
            if "tistory.com" in domain:
                blog_name = domain.replace(".tistory.com", "").replace("www.", "")
                return f"{blog_name} (í‹°ìŠ¤í† ë¦¬)"

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸
            if "blog.naver.com" in domain:
                path_parts = parsed_url.path.split("/")
                if len(path_parts) > 1:
                    blog_id = path_parts[1]
                    return f"{blog_id} (ë„¤ì´ë²„ ë¸”ë¡œê·¸)"
                return "ë„¤ì´ë²„ ë¸”ë¡œê·¸"

            # ë¸ŒëŸ°ì¹˜
            if "brunch.co.kr" in domain:
                path_parts = parsed_url.path.split("/")
                if len(path_parts) > 1 and path_parts[0] == "":
                    blog_id = path_parts[1].replace("@", "")
                    return f"@{blog_id} (ë¸ŒëŸ°ì¹˜)"
                return "ë¸ŒëŸ°ì¹˜"

            # ë¯¸ë””ì—„
            if "medium.com" in domain:
                path_parts = parsed_url.path.split("/")
                if len(path_parts) > 1 and path_parts[1].startswith("@"):
                    blog_id = path_parts[1]
                    return f"{blog_id} (ë¯¸ë””ì—„)"
                return "ë¯¸ë””ì—„"

            # ê¸°íƒ€ ë¸”ë¡œê·¸
            if any(
                keyword in domain
                for keyword in ["blog", "diary", "note", "post", "story"]
            ):
                return f"{domain.replace('www.', '')} (ë¸”ë¡œê·¸)"

        except Exception:
            pass

    # 3. ì œëª©ì´ë‚˜ ì„¤ëª…ì—ì„œ ë¸”ë¡œê·¸ëª… ì¶”ì¶œ ì‹œë„
    combined_text = f"{title} {description}"

    # ê´„í˜¸ ì•ˆì˜ ë¸”ë¡œê·¸ëª…
    bracket_source = re.search(r"\(([ê°€-í£A-Za-z0-9]+\s*ë¸”ë¡œê·¸)\)", combined_text)
    if bracket_source:
        return bracket_source.group(1)

    return "ë¸”ë¡œê·¸"


def _get_real_url_from_google(google_url: str) -> str:
    """
    Google ê²€ìƒ‰ URLì—ì„œ ì‹¤ì œ ì›ë³¸ ë¸”ë¡œê·¸ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        google_url (str): Google ê²€ìƒ‰ URL

    Returns:
        str: ì‹¤ì œ ë¸”ë¡œê·¸ URL ë˜ëŠ” ì›ë³¸ URL
    """
    try:
        if not google_url or "google.com" not in google_url:
            return google_url

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        # Google ë§í¬ë¥¼ ë”°ë¼ê°€ì„œ ì‹¤ì œ URL ì–»ê¸°
        response = requests.get(
            google_url, headers=headers, timeout=10, allow_redirects=True
        )

        # ìµœì¢… ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ URL ë°˜í™˜
        final_url = response.url

        # Google ë‚´ë¶€ URLì´ë©´ í¬ê¸°
        if "google.com" in final_url:
            return google_url

        return final_url

    except Exception:
        return google_url


def _fetch_blog_content(url: str) -> str:
    """
    ë¸”ë¡œê·¸ URLì—ì„œ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        url (str): ë¸”ë¡œê·¸ URL

    Returns:
        str: ì¶”ì¶œëœ ë³¸ë¬¸ ë‚´ìš©
    """
    try:
        # Google URLì¸ ê²½ìš° ì‹¤ì œ URLë¡œ ë³€í™˜ ì‹œë„
        real_url = _get_real_url_from_google(url)

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        response = requests.get(real_url, headers=headers, timeout=10)
        if response.status_code != 200:
            return ""

        soup = BeautifulSoup(response.content, "html.parser")

        # ê´‘ê³ , ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
        for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
            script.decompose()

        # ë‹¤ì–‘í•œ ë¸”ë¡œê·¸ í”Œë«í¼ì˜ ë³¸ë¬¸ ì„ íƒì
        content_selectors = [
            # í‹°ìŠ¤í† ë¦¬
            ".entry-content",
            ".tt_article_useless_p_margin",
            ".article-content",
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸
            "#postViewArea",
            ".se-main-container",
            ".se_component_wrap",
            # ë¸ŒëŸ°ì¹˜
            ".wrap_body",
            ".wrap_view_article",
            # ë¯¸ë””ì—„
            "article",
            ".postArticle-content",
            # ì¼ë°˜ì ì¸ ì„ íƒì
            ".post-content",
            ".blog-content",
            ".content",
            'div[class*="content"]',
            'div[class*="post"]',
            'div[class*="article"]',
            # ë§ˆì§€ë§‰ ì‹œë„: p íƒœê·¸ë“¤
            "p",
        ]

        content = ""
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                if selector == "p":
                    # p íƒœê·¸ì˜ ê²½ìš° ì—¬ëŸ¬ ê°œë¥¼ í•©ì¹¨
                    paragraphs = []
                    for elem in elements[:10]:  # ì²˜ìŒ 10ê°œ ë¬¸ë‹¨ê¹Œì§€
                        text = elem.get_text().strip()
                        if (
                            len(text) > 20
                            and not text.startswith(("ì‚¬ì§„", "ì´ë¯¸ì§€", "ì¶œì²˜", "Â©"))
                            and "ê´‘ê³ " not in text
                            and "í™ë³´" not in text
                        ):
                            paragraphs.append(text)
                    content = " ".join(paragraphs)
                else:
                    content = elements[0].get_text().strip()

                if len(content) > 100:  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                    break

        # í…ìŠ¤íŠ¸ ì •ì œ
        if content:
            import re

            # ì—°ì†ëœ ê³µë°±, ì¤„ë°”ê¿ˆ ì •ë¦¬
            content = re.sub(r"\s+", " ", content).strip()

            # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ì²˜ìŒ 500ìë§Œ ì‚¬ìš©
            if len(content) > 500:
                content = content[:500]

        return content

    except Exception:
        return ""


def _generate_summary(title: str, content: str, description: str = "") -> str:
    """
    ê¸€ ì œëª©, ë³¸ë¬¸, ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        title (str): ê¸€ ì œëª©
        content (str): ê¸€ ë³¸ë¬¸
        description (str): ê¸€ ì„¤ëª…

    Returns:
        str: ìƒì„±ëœ ìš”ì•½
    """
    import re

    # ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ë³¸ë¬¸ ê¸°ë°˜ ìš”ì•½ ìƒì„±
    if content and len(content.strip()) > 50:
        # ë³¸ë¬¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        sentences = re.split(r"[.!?]", content)
        meaningful_sentences = []

        # ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        title_keywords = set(re.findall(r"[ê°€-í£]{2,}", title))

        for sentence in sentences:
            sentence = sentence.strip()

            # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ í•„í„°ë§
            if (
                len(sentence) > 20
                and len(sentence) < 200
                and not sentence.startswith(("ì‚¬ì§„", "ì´ë¯¸ì§€", "ì¶œì²˜", "Â©"))
                and not re.match(r"^[\d\s\-()]+$", sentence)
                and "ê´‘ê³ " not in sentence
                and "í™ë³´" not in sentence
            ):
                # ì œëª©ê³¼ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì¥ ìš°ì„  ì„ íƒ
                sentence_keywords = set(re.findall(r"[ê°€-í£]{2,}", sentence))
                relevance_score = len(title_keywords.intersection(sentence_keywords))

                meaningful_sentences.append((sentence, relevance_score))

        if meaningful_sentences:
            # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
            meaningful_sentences.sort(key=lambda x: x[1], reverse=True)

            # ìƒìœ„ 2ê°œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ êµ¬ì„±
            selected_sentences = [sent[0] for sent in meaningful_sentences[:2]]
            summary = ". ".join(selected_sentences)

            # ìš”ì•½ì´ ì œëª©ê³¼ ë„ˆë¬´ ìœ ì‚¬í•˜ë©´ ë‹¤ë¥¸ ë¬¸ì¥ ì‹œë„
            if (
                _calculate_similarity(
                    _normalize_title(title), _normalize_title(summary)
                )
                > 0.7
            ):
                if len(meaningful_sentences) > 2:
                    selected_sentences = [sent[0] for sent in meaningful_sentences[1:3]]
                    summary = ". ".join(selected_sentences)

            if len(summary) > 150:
                summary = summary[:147] + "..."
            elif not summary.endswith("."):
                summary += "."

            return summary

    # ë³¸ë¬¸ì´ ì—†ê±°ë‚˜ ì§§ìœ¼ë©´ ì„¤ëª…ì—ì„œ ìš”ì•½ ì¶”ì¶œ ì‹œë„
    if description:
        cleaned_desc = _clean_description(description)
        if (
            cleaned_desc
            and cleaned_desc
            not in ["ìš”ì•½ ì •ë³´ ì—†ìŒ", "ë¸”ë¡œê·¸ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”"]
            and _calculate_similarity(
                _normalize_title(title), _normalize_title(cleaned_desc)
            )
            < 0.8
        ):
            return cleaned_desc

    return "ë¸”ë¡œê·¸ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”."


def format_blog_info_text(blog_list: List[Dict[str, Any]], keyword: str) -> str:
    """
    ë¸”ë¡œê·¸ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.

    Args:
        blog_list (List[Dict[str, Any]]): ë¸”ë¡œê·¸ ê¸€ ëª©ë¡
        keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ

    Returns:
        str: í¬ë§·íŒ…ëœ ë¸”ë¡œê·¸ ì •ë³´ í…ìŠ¤íŠ¸
    """
    if not blog_list:
        return f"""
"{keyword}" ê´€ë ¨ ìµœì‹  ë¸”ë¡œê·¸:
- ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.
"""

    result = f"""
"{keyword}" ê´€ë ¨ ìµœì‹  ë¸”ë¡œê·¸ ({len(blog_list)}ê±´):

"""

    for i, blog in enumerate(blog_list, 1):
        result += f"{i}. {blog.get('title', 'ì œëª© ì—†ìŒ')}\n"
        result += f"   ìš”ì•½: {blog.get('summary', 'ìš”ì•½ ì—†ìŒ')}\n"
        result += f"   ì¶œì²˜: {blog.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
        result += f"   ë“±ë¡: {blog.get('pub_date', 'ì‹œê°„ ì •ë³´ ì—†ìŒ')}\n"

        if blog.get("link"):
            result += f"   ë§í¬: {blog['link']}\n"

        result += "\n"

    return result


def format_blog_info_html(blog_list: List[Dict[str, Any]], keyword: str) -> str:
    """
    ë¸”ë¡œê·¸ ì •ë³´ë¥¼ HTML í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.

    Args:
        blog_list (List[Dict[str, Any]]): ë¸”ë¡œê·¸ ê¸€ ëª©ë¡
        keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ

    Returns:
        str: í¬ë§·íŒ…ëœ ë¸”ë¡œê·¸ ì •ë³´ HTML
    """
    if not blog_list:
        return f"""
<h3>âœï¸ "{keyword}" ê´€ë ¨ ìµœì‹  ë¸”ë¡œê·¸</h3>
<p><em>ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.</em></p>
"""

    result = f"""
<h3>âœï¸ "{keyword}" ê´€ë ¨ ìµœì‹  ë¸”ë¡œê·¸ ({len(blog_list)}ê±´)</h3>
<div style="margin-left: 10px;">
"""

    for i, blog in enumerate(blog_list, 1):
        title = blog.get("title", "ì œëª© ì—†ìŒ")
        summary = blog.get("summary", "ìš”ì•½ ì—†ìŒ")
        source = blog.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
        pub_date = blog.get("pub_date", "ì‹œê°„ ì •ë³´ ì—†ìŒ")
        link = blog.get("link", "")

        result += f"""
<div style="margin-bottom: 15px; padding: 10px; border-left: 3px solid #28a745;">
    <h4 style="margin: 0 0 5px 0; color: #28a745;">
        {i}. {title}
    </h4>
    <p style="margin: 5px 0; color: #555; font-size: 14px;">
        ğŸ“ <strong>ìš”ì•½:</strong> {summary}
    </p>
    <p style="margin: 5px 0; color: #777; font-size: 12px;">
        âœï¸ <strong>ì¶œì²˜:</strong> {source} |
        ğŸ•’ <strong>ë“±ë¡:</strong> {pub_date}
    </p>
"""

        if link:
            result += f"""
    <p style="margin: 5px 0; font-size: 12px;">
        ğŸ”— <a href="{link}" style="color: #28a745;">ë¸”ë¡œê·¸ ë§í¬</a>
    </p>
"""

        result += "</div>\n"

    result += "</div>\n"

    return result
