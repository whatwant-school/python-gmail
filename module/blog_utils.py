"""
Blog utility functions for searching latest blog posts by keyword
"""

import os
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import requests


def search_blogs_by_keyword(
    keyword: str, max_results: int = 5, hours_back: int = 24
) -> List[Dict[str, Any]]:
    """
    í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœê·¼ ë¸”ë¡œê·¸ ê¸€ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. (Naver API ì‚¬ìš©)

    Args:
        keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜ (3-5ê°œ ê¶Œì¥, ìµœëŒ€ 100)
        hours_back (int): ê²€ìƒ‰í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)

    Returns:
        List[Dict[str, Any]]: ë¸”ë¡œê·¸ ê¸€ ëª©ë¡

    Environment Variables:
        NAVER_CLIENT_ID: Naver API Client ID
        NAVER_CLIENT_SECRET: Naver API Client Secret
    """
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ Naver API ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        client_id = os.getenv("NAVER_CLIENT_ID")
        client_secret = os.getenv("NAVER_CLIENT_SECRET")

        if not client_id or not client_secret:
            print(
                "ê²½ê³ : NAVER_CLIENT_ID ë˜ëŠ” NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
            return _fallback_blog_search(keyword, max_results)

        # Naver ë¸”ë¡œê·¸ ê²€ìƒ‰ API í˜¸ì¶œ
        import urllib.parse

        # í‚¤ì›Œë“œë¥¼ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        keywords = keyword.strip().split()
        all_articles = []

        if len(keywords) > 1:
            # ì—¬ëŸ¬ í‚¤ì›Œë“œì¸ ê²½ìš° ê°ê° ê°œë³„ ê²€ìƒ‰ í›„ í•©ì¹˜ê¸°
            for kw in keywords:
                search_query = f"{kw} -ê´‘ê³  -í™ë³´"
                encoded_query = urllib.parse.quote(search_query)
                api_url = f"https://openapi.naver.com/v1/search/blog.json?query={encoded_query}&display={min(max_results * 3, 100)}&sort=date"

                headers = {
                    "X-Naver-Client-Id": client_id,
                    "X-Naver-Client-Secret": client_secret,
                }

                response = requests.get(api_url, headers=headers, timeout=10)

                if response.status_code == 200:
                    data = response.json()
                    all_articles.extend(data.get("items", []))
        else:
            # ë‹¨ì¼ í‚¤ì›Œë“œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            search_query = f"{keyword} -ê´‘ê³  -í™ë³´"
            encoded_query = urllib.parse.quote(search_query)
            api_url = f"https://openapi.naver.com/v1/search/blog.json?query={encoded_query}&display={min(max_results * 2, 100)}&sort=date"

            headers = {
                "X-Naver-Client-Id": client_id,
                "X-Naver-Client-Secret": client_secret,
            }

            response = requests.get(api_url, headers=headers, timeout=10)

            if response.status_code != 200:
                print(f"Naver API ì˜¤ë¥˜: {response.status_code}")
                print(f"ì‘ë‹µ ë‚´ìš©: {response.text[:200]}")
                return _fallback_blog_search(keyword, max_results)

            data = response.json()
            all_articles = data.get("items", [])

        articles = all_articles

        if not articles:
            return _fallback_blog_search(keyword, max_results)

        if not articles:
            return _fallback_blog_search(keyword, max_results)

        # ì‹œê°„ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°
        # timezone awareë¡œ ë³€ê²½ (í•œêµ­ ì‹œê°„ ê¸°ì¤€)
        from datetime import timezone

        # KST (UTC+9) ê¸°ì¤€ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ê³„ì‚°
        kst = timezone(timedelta(hours=9))
        cutoff_time = datetime.now(kst) - timedelta(hours=hours_back)
        filtered_blogs = []
        seen_titles = set()

        for article in articles:
            # ì œëª© ì¤‘ë³µ ì²´í¬ (ìœ ì‚¬í•œ ê¸€ ì œì™¸)
            title = article.get("title", "")
            # HTML íƒœê·¸ ì œê±°
            title = _remove_html_tags(title)

            if not title or _is_similar_title(title, seen_titles):
                continue

            # ê´‘ê³ /í™ë³´ ê¸€ í•„í„°ë§
            description = article.get("description", "")
            description = _remove_html_tags(description)

            if _is_ad_or_promotional(title, description):
                continue

            # ë°œí–‰ ì‹œê°„ íŒŒì‹± (Naver APIëŠ” "YYYYMMDD" í˜•ì‹)
            pub_date = _parse_pub_date(article.get("postdate", ""))
            if pub_date and pub_date < cutoff_time:
                continue

            # ë¸”ë¡œê·¸ ì •ë³´ êµ¬ì„±
            link = article.get("link", "")

            # ë¸”ë¡œê·¸ ì¶œì²˜ ì¶”ì¶œ
            source = _extract_blog_source(
                link, article.get("bloggername", ""), article.get("bloggerlink", "")
            )

            # ìš”ì•½ ìƒì„±
            summary = (
                _clean_description(description) if description else "ìš”ì•½ ì •ë³´ ì—†ìŒ"
            )

            blog_item = {
                "title": title,
                "summary": summary,
                "source": source,
                "link": link,
                "pub_date": pub_date.strftime("%Y-%m-%d %H:%M")
                if pub_date
                else "ì‹œê°„ ì •ë³´ ì—†ìŒ",
                "pub_date_raw": pub_date,
            }

            filtered_blogs.append(blog_item)
            seen_titles.add(_normalize_title(title))

            if len(filtered_blogs) >= max_results:
                break

        # ë°œí–‰ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        filtered_blogs.sort(
            key=lambda x: x.get("pub_date_raw") or datetime.min, reverse=True
        )

        # ìµœì¢… ê²°ê³¼ì—ì„œ raw ì‹œê°„ ì •ë³´ ì œê±°
        for blog in filtered_blogs:
            blog.pop("pub_date_raw", None)

        return filtered_blogs[:max_results]

    except Exception as e:
        print(f"ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return _fallback_blog_search(keyword, max_results)


def _remove_html_tags(text: str) -> str:
    """
    HTML íƒœê·¸ì™€ ì—”í‹°í‹°ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

    Args:
        text (str): HTMLì´ í¬í•¨ëœ í…ìŠ¤íŠ¸

    Returns:
        str: HTML íƒœê·¸ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸
    """
    import html
    import re

    if not text:
        return ""

    # HTML ì—”í‹°í‹° ë””ì½”ë“œ (&lt;, &gt;, &quot; ë“±)
    text = html.unescape(text)

    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r"<[^>]+>", "", text)

    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text).strip()

    return text


def _fallback_blog_search(keyword: str, max_results: int = 5) -> List[Dict[str, Any]]:
    """
    ì£¼ APIê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ ì‚¬ìš©í•  ëŒ€ì²´ ë¸”ë¡œê·¸ ê²€ìƒ‰ ë°©ë²•

    Args:
        keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        List[Dict[str, Any]]: ë¸”ë¡œê·¸ ê¸€ ëª©ë¡
    """
    # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš°
    return [
        {
            "title": f"{keyword} ë¸”ë¡œê·¸ ê²€ìƒ‰ ì‹¤íŒ¨",
            "summary": "ë„¤íŠ¸ì›Œí¬ ë¬¸ì œë‚˜ API ì„œë¹„ìŠ¤ ì¥ì• ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "source": "ì‹œìŠ¤í…œ ì˜¤ë¥˜",
            "link": "",
            "pub_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
        }
    ]


def _parse_pub_date(date_str: str) -> Optional[datetime]:
    """
    ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜

    Args:
        date_str (str): ë‚ ì§œ ë¬¸ìì—´ (Naver APIëŠ” "YYYYMMDD" í˜•ì‹)

    Returns:
        Optional[datetime]: íŒŒì‹±ëœ datetime ê°ì²´ ë˜ëŠ” None
    """
    if not date_str:
        return None

    # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
    formats = [
        "%Y%m%d",  # Naver API í˜•ì‹ (YYYYMMDD)
        "%a, %d %b %Y %H:%M:%S %Z",  # RSS í‘œì¤€ í˜•ì‹
        "%a, %d %b %Y %H:%M:%S %z",  # RFC 2822
        "%Y-%m-%dT%H:%M:%S%z",  # ISO 8601
        "%Y-%m-%d %H:%M:%S",  # ì¼ë°˜ì ì¸ í˜•ì‹
        "%Y-%m-%d",  # ë‚ ì§œë§Œ
    ]

    for fmt in formats:
        try:
            parsed_date = datetime.strptime(date_str.strip(), fmt)
            # timezoneì´ ì—†ìœ¼ë©´ KST ì¶”ê°€
            if parsed_date.tzinfo is None:
                from datetime import timezone

                kst = timezone(timedelta(hours=9))
                parsed_date = parsed_date.replace(tzinfo=kst)
            return parsed_date
        except ValueError:
            continue

    # í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ë„ ì‹œë„
    try:
        # "2024ë…„ 1ì›” 15ì¼" í˜•ì‹ ì²˜ë¦¬
        import re

        korean_date = re.search(r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼", date_str)
        if korean_date:
            year, month, day = korean_date.groups()
            from datetime import timezone

            kst = timezone(timedelta(hours=9))
            return datetime(int(year), int(month), int(day), tzinfo=kst)
    except Exception as e:
        import logging

        logging.warning(f"_parse_pub_date ì˜ˆì™¸: {e}")

    return None


def _is_similar_title(title: str, seen_titles: set) -> bool:
    """
    ì œëª©ì´ ì´ë¯¸ ë³¸ ì œëª©ë“¤ê³¼ ìœ ì‚¬í•œì§€ í™•ì¸

    Args:
        title (str): í™•ì¸í•  ì œëª©
        seen_titles (set): ì´ë¯¸ ë³¸ ì œëª©ë“¤ì˜ ì§‘í•©

    Returns:
        bool: ìœ ì‚¬í•œ ì œëª©ì´ ìˆìœ¼ë©´ True
    """
    normalized_title = _normalize_title(title)

    for seen_title in seen_titles:
        # 50% ì´ìƒ ê²¹ì¹˜ë©´ ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ íŒë‹¨
        if _calculate_similarity(normalized_title, seen_title) > 0.5:
            return True

    return False


def _normalize_title(title: str) -> str:
    """
    ì œëª©ì„ ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)

    Args:
        title (str): ì›ë³¸ ì œëª©

    Returns:
        str: ì •ê·œí™”ëœ ì œëª©
    """
    import re

    # íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜
    normalized = re.sub(r"[^\w\sê°€-í£]", "", title)
    normalized = re.sub(r"\s+", " ", normalized).strip().lower()
    return normalized


def _calculate_similarity(str1: str, str2: str) -> float:
    """
    ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)

    Args:
        str1 (str): ì²« ë²ˆì§¸ ë¬¸ìì—´
        str2 (str): ë‘ ë²ˆì§¸ ë¬¸ìì—´

    Returns:
        float: ìœ ì‚¬ë„ (0.0 ~ 1.0)
    """
    words1 = set(str1.split())
    words2 = set(str2.split())

    if not words1 and not words2:
        return 1.0

    intersection = words1.intersection(words2)
    union = words1.union(words2)

    return len(intersection) / len(union) if union else 0.0


def _is_ad_or_promotional(title: str, description: str) -> bool:
    """
    ê´‘ê³ ë‚˜ í™ë³´ì„± ê¸€ì¸ì§€ í™•ì¸

    Args:
        title (str): ê¸€ ì œëª©
        description (str): ê¸€ ì„¤ëª…

    Returns:
        bool: ê´‘ê³ /í™ë³´ ê¸€ì´ë©´ True
    """
    # ì œëª©ê³¼ ì„¤ëª…ì„ í•©ì¹œ í…ìŠ¤íŠ¸ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
    combined = (title + " " + description).lower()

    # ëª…í™•í•œ ê´‘ê³ ì„± í‘œì‹œë§Œ ì²´í¬ (ì œëª©ì´ë‚˜ ì„¤ëª… ì•ë¶€ë¶„ì— í‘œì‹œëœ ê²½ìš°)
    ad_keywords = [
        "[ê´‘ê³ ]",
        "(ê´‘ê³ )",
        "[pr]",
        "(pr)",
        "[í™ë³´]",
        "(í™ë³´)",
        "ì œíœ´ê´‘ê³ ",
        "í˜‘ì°¬ê´‘ê³ ",
    ]

    # ì œëª©ì´ë‚˜ ì„¤ëª…ì˜ ì‹œì‘ 100ì ì´ë‚´ì— ê´‘ê³  í‘œì‹œê°€ ìˆëŠ”ì§€ í™•ì¸
    prefix = combined[:100]

    return any(keyword in prefix for keyword in ad_keywords)


def _clean_description(description: str) -> str:
    """
    ê¸€ ì„¤ëª…ì—ì„œ HTML íƒœê·¸ë‚˜ ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°í•˜ê³  ì‹¤ì œ ë³¸ë¬¸ ìš”ì•½ ìƒì„±

    Args:
        description (str): ì›ë³¸ ì„¤ëª…

    Returns:
        str: ì •ì œëœ ì„¤ëª…
    """
    import re

    if not description:
        return "ìš”ì•½ ì •ë³´ ì—†ìŒ"

    # HTML íƒœê·¸ ì œê±°
    clean_desc = re.sub(r"<[^>]+>", "", description)

    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    clean_desc = re.sub(r"\s+", " ", clean_desc).strip()

    # ë¸”ë¡œê·¸ ì¶œì²˜ ì •ë³´ë‚˜ ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì œê±°
    clean_desc = re.sub(
        r"\s*-\s*[ê°€-í£A-Za-z0-9]+(?:ë¸”ë¡œê·¸|BLOG|Blog)?\s*$", "", clean_desc
    )

    # URLì´ë‚˜ ë„ë©”ì¸ ì •ë³´ ì œê±°
    clean_desc = re.sub(r"https?://[^\s]+", "", clean_desc)
    clean_desc = re.sub(r"www\.[^\s]+", "", clean_desc)
    clean_desc = re.sub(
        r"[a-zA-Z0-9.-]+\.(com|net|co\.kr|org|tistory\.com|naver\.com)\b",
        "",
        clean_desc,
    )

    # ë„ˆë¬´ ì§§ì€ ì„¤ëª…
    if len(clean_desc.strip()) < 20:
        return "ë¸”ë¡œê·¸ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”"

    # ì—°ì†ëœ ê³µë°± ë‹¤ì‹œ ì •ë¦¬
    clean_desc = re.sub(r"\s+", " ", clean_desc).strip()

    # ë„ˆë¬´ ê¸´ ì„¤ëª…ì€ ì˜ë¼ë‚´ê¸° (150ì ì œí•œ)
    if len(clean_desc) > 150:
        sentences = clean_desc.split(". ")
        if len(sentences) > 1:
            clean_desc = sentences[0] + "."
            if len(clean_desc) > 150:
                clean_desc = clean_desc[:147] + "..."
        else:
            clean_desc = clean_desc[:147] + "..."

    return (
        clean_desc if clean_desc.strip() else "ë¸”ë¡œê·¸ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”"
    )


def _extract_blog_source(
    link: str, blogger_name: str = "", blogger_link: str = ""
) -> str:
    """
    ë¸”ë¡œê·¸ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ

    Args:
        link (str): ë¸”ë¡œê·¸ ë§í¬
        blogger_name (str): ë¸”ë¡œê±° ì´ë¦„ (Naver API ì œê³µ)
        blogger_link (str): ë¸”ë¡œê±° ë§í¬ (Naver API ì œê³µ)

    Returns:
        str: ì¶œì²˜ ì´ë¦„
    """
    from urllib.parse import urlparse

    # 1. Naver APIì—ì„œ ì œê³µí•˜ëŠ” ë¸”ë¡œê±° ì´ë¦„ ì‚¬ìš©
    if blogger_name:
        # ë¸”ë¡œê·¸ í”Œë«í¼ íŒë³„
        platform = ""
        if link:
            if "blog.naver.com" in link:
                platform = " (ë„¤ì´ë²„ ë¸”ë¡œê·¸)"
            elif "tistory.com" in link:
                platform = " (í‹°ìŠ¤í† ë¦¬)"
            elif "brunch.co.kr" in link:
                platform = " (ë¸ŒëŸ°ì¹˜)"

        return f"{blogger_name}{platform}"

    # 2. ë§í¬ì—ì„œ ë¸”ë¡œê·¸ í”Œë«í¼ ë° ì‚¬ìš©ì ì¶”ì¶œ
    if link:
        try:
            parsed_url = urlparse(link)
            domain = parsed_url.netloc.lower()

            # í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸
            if "tistory.com" in domain:
                blog_name = domain.replace(".tistory.com", "").replace("www.", "")
                return f"{blog_name} (í‹°ìŠ¤í† ë¦¬)"

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸
            if "blog.naver.com" in domain:
                path_parts = parsed_url.path.split("/")
                if len(path_parts) > 1:
                    blog_id = path_parts[1]
                    return f"{blog_id} (ë„¤ì´ë²„ ë¸”ë¡œê·¸)"
                return "ë„¤ì´ë²„ ë¸”ë¡œê·¸"

            # ë¸ŒëŸ°ì¹˜
            if "brunch.co.kr" in domain:
                path_parts = parsed_url.path.split("/")
                if len(path_parts) > 1 and path_parts[0] == "":
                    blog_id = path_parts[1].replace("@", "")
                    return f"@{blog_id} (ë¸ŒëŸ°ì¹˜)"
                return "ë¸ŒëŸ°ì¹˜"

            # ë¯¸ë””ì—„
            if "medium.com" in domain:
                path_parts = parsed_url.path.split("/")
                if len(path_parts) > 1 and path_parts[1].startswith("@"):
                    blog_id = path_parts[1]
                    return f"{blog_id} (ë¯¸ë””ì—„)"
                return "ë¯¸ë””ì—„"

            # ê¸°íƒ€ ë¸”ë¡œê·¸
            if any(
                keyword in domain
                for keyword in ["blog", "diary", "note", "post", "story"]
            ):
                return f"{domain.replace('www.', '')} (ë¸”ë¡œê·¸)"

        except Exception as e:
            import logging

            logging.warning(f"_extract_blog_source ì˜ˆì™¸: {e}")

    return "ë¸”ë¡œê·¸"


def format_blog_info_text(blog_list: List[Dict[str, Any]], keyword: str) -> str:
    """
    ë¸”ë¡œê·¸ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.

    Args:
        blog_list (List[Dict[str, Any]]): ë¸”ë¡œê·¸ ê¸€ ëª©ë¡
        keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ

    Returns:
        str: í¬ë§·íŒ…ëœ ë¸”ë¡œê·¸ ì •ë³´ í…ìŠ¤íŠ¸
    """
    if not blog_list:
        return f"""
"{keyword}" ê´€ë ¨ ìµœì‹  ë¸”ë¡œê·¸:
- ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.
"""

    result = f"""
"{keyword}" ê´€ë ¨ ìµœì‹  ë¸”ë¡œê·¸ ({len(blog_list)}ê±´):

"""

    for i, blog in enumerate(blog_list, 1):
        result += f"{i}. {blog.get('title', 'ì œëª© ì—†ìŒ')}\n"
        result += f"   ìš”ì•½: {blog.get('summary', 'ìš”ì•½ ì—†ìŒ')}\n"
        result += f"   ì¶œì²˜: {blog.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
        result += f"   ë“±ë¡: {blog.get('pub_date', 'ì‹œê°„ ì •ë³´ ì—†ìŒ')}\n"

        if blog.get("link"):
            result += f"   ë§í¬: {blog['link']}\n"

        result += "\n"

    return result


def format_blog_info_html(blog_list: List[Dict[str, Any]], keyword: str) -> str:
    """
    ë¸”ë¡œê·¸ ì •ë³´ë¥¼ HTML í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.

    Args:
        blog_list (List[Dict[str, Any]]): ë¸”ë¡œê·¸ ê¸€ ëª©ë¡
        keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ

    Returns:
        str: í¬ë§·íŒ…ëœ ë¸”ë¡œê·¸ ì •ë³´ HTML
    """
    if not blog_list:
        return f"""
<h3>âœï¸ "{keyword}" ê´€ë ¨ ìµœì‹  ë¸”ë¡œê·¸</h3>
<p><em>ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.</em></p>
"""

    result = f"""
<h3>âœï¸ "{keyword}" ê´€ë ¨ ìµœì‹  ë¸”ë¡œê·¸ ({len(blog_list)}ê±´)</h3>
<div style="margin-left: 10px;">
"""

    for i, blog in enumerate(blog_list, 1):
        title = blog.get("title", "ì œëª© ì—†ìŒ")
        summary = blog.get("summary", "ìš”ì•½ ì—†ìŒ")
        source = blog.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
        pub_date = blog.get("pub_date", "ì‹œê°„ ì •ë³´ ì—†ìŒ")
        link = blog.get("link", "")

        result += f"""
<div style="margin-bottom: 15px; padding: 10px; border-left: 3px solid #28a745;">
    <h4 style="margin: 0 0 5px 0; color: #28a745;">
        {i}. {title}
    </h4>
    <p style="margin: 5px 0; color: #555; font-size: 14px;">
        ğŸ“ <strong>ìš”ì•½:</strong> {summary}
    </p>
    <p style="margin: 5px 0; color: #777; font-size: 12px;">
        âœï¸ <strong>ì¶œì²˜:</strong> {source} |
        ğŸ•’ <strong>ë“±ë¡:</strong> {pub_date}
    </p>
"""

        if link:
            result += f"""
    <p style="margin: 5px 0; font-size: 12px;">
        ğŸ”— <a href="{link}" style="color: #28a745;">ë¸”ë¡œê·¸ ë§í¬</a>
    </p>
"""

        result += "</div>\n"

    result += "</div>\n"

    return result
