"""
News utility functions for searching latest news by keyword
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import requests
from bs4 import BeautifulSoup


def search_news_by_keyword(
    keyword: str, max_results: int = 5, hours_back: int = 24
) -> List[Dict[str, Any]]:
    """
    í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœê·¼ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜ (3-5ê°œ ê¶Œì¥)
        hours_back (int): ê²€ìƒ‰í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„ ë‹¨ìœ„, ê¸°ë³¸ 24ì‹œê°„)

    Returns:
        List[Dict[str, Any]]: ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡
    """
    try:
        # Google News RSS APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê²€ìƒ‰
        # RSSë¥¼ JSONìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë¬´ë£Œ ì„œë¹„ìŠ¤ í™œìš©
        import urllib.parse

        search_query = f"{keyword} -ê´‘ê³  -í™ë³´"  # ê´‘ê³ , í™ë³´ ì œì™¸
        encoded_query = urllib.parse.quote(search_query)

        # RSS2JSON API ì‚¬ìš© (ë¬´ë£Œ ì„œë¹„ìŠ¤)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        encoded_rss_url = urllib.parse.quote(rss_url, safe="")
        api_url = f"https://api.rss2json.com/v1/api.json?rss_url={encoded_rss_url}"

        response = requests.get(api_url, timeout=10)

        if response.status_code != 200:
            return _fallback_news_search(keyword, max_results)

        data = response.json()

        if data.get("status") != "ok":
            return _fallback_news_search(keyword, max_results)

        articles = data.get("items", [])

        # ì‹œê°„ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        filtered_articles = []
        seen_titles = set()

        for article in articles:
            # ì œëª© ì¤‘ë³µ ì²´í¬ (ìœ ì‚¬í•œ ê¸°ì‚¬ ì œì™¸)
            title = article.get("title", "")
            if not title or _is_similar_title(title, seen_titles):
                continue

            # ê´‘ê³ /í™ë³´ ê¸°ì‚¬ í•„í„°ë§
            if _is_ad_or_promotional(title, article.get("description", "")):
                continue

            # ë°œí–‰ ì‹œê°„ íŒŒì‹±
            pub_date = _parse_pub_date(article.get("pubDate", ""))
            if pub_date and pub_date < cutoff_time:
                continue

            # ë‰´ìŠ¤ ì •ë³´ êµ¬ì„±
            description = article.get("description", "")
            link = article.get("link", "")

            # ê°œì„ ëœ ì¶œì²˜ ì¶”ì¶œ
            source = _extract_source(article.get("source", {}))
            if source == "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜":
                source = _extract_source_from_title_and_description(
                    title, description, link
                )

            # ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì™€ì„œ ê°œì„ ëœ ìš”ì•½ ìƒì„±
            try:
                article_content = _fetch_article_content(link) if link else ""
                summary = _generate_summary(title, article_content, description)
            except Exception:
                summary = _clean_description(description)

            news_item = {
                "title": title,
                "summary": summary,
                "source": source,
                "link": link,
                "pub_date": pub_date.strftime("%Y-%m-%d %H:%M")
                if pub_date
                else "ì‹œê°„ ì •ë³´ ì—†ìŒ",
                "pub_date_raw": pub_date,
            }

            filtered_articles.append(news_item)
            seen_titles.add(_normalize_title(title))

            if len(filtered_articles) >= max_results:
                break

        # ë°œí–‰ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        filtered_articles.sort(
            key=lambda x: x.get("pub_date_raw") or datetime.min, reverse=True
        )

        # ìµœì¢… ê²°ê³¼ì—ì„œ raw ì‹œê°„ ì •ë³´ ì œê±°
        for article in filtered_articles:
            article.pop("pub_date_raw", None)

        return filtered_articles[:max_results]

    except Exception as e:
        print(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return _fallback_news_search(keyword, max_results)


def _fallback_news_search(keyword: str, max_results: int = 5) -> List[Dict[str, Any]]:
    """
    ì£¼ APIê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ ì‚¬ìš©í•  ëŒ€ì²´ ë‰´ìŠ¤ ê²€ìƒ‰ ë°©ë²•

    Args:
        keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        List[Dict[str, Any]]: ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡
    """
    try:
        # ë„¤ì´ë²„ ë‰´ìŠ¤ RSSë¥¼ ëŒ€ì²´ë¡œ ì‚¬ìš©
        naver_rss_url = "https://rss.donga.com/total.xml"

        response = requests.get(naver_rss_url, timeout=10)
        if response.status_code == 200:
            # ê°„ë‹¨í•œ XML íŒŒì‹±ìœ¼ë¡œ ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ (í–¥í›„ êµ¬í˜„ ì˜ˆì •)
            # ì„ì‹œ ê²°ê³¼ ë°˜í™˜
            return [
                {
                    "title": f"{keyword} ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "summary": "ë‰´ìŠ¤ API ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "source": "ì‹œìŠ¤í…œ ì•Œë¦¼",
                    "link": "",
                    "pub_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
                }
            ]

    except Exception:
        pass

    # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš°
    return [
        {
            "title": f"{keyword} ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨",
            "summary": "ë„¤íŠ¸ì›Œí¬ ë¬¸ì œë‚˜ API ì„œë¹„ìŠ¤ ì¥ì• ë¡œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "source": "ì‹œìŠ¤í…œ ì˜¤ë¥˜",
            "link": "",
            "pub_date": datetime.now().strftime("%Y-%m-%d %H:%M"),
        }
    ]


def _parse_pub_date(date_str: str) -> Optional[datetime]:
    """
    ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜

    Args:
        date_str (str): ë‚ ì§œ ë¬¸ìì—´

    Returns:
        Optional[datetime]: íŒŒì‹±ëœ datetime ê°ì²´ ë˜ëŠ” None
    """
    if not date_str:
        return None

    # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
    formats = [
        "%a, %d %b %Y %H:%M:%S %Z",  # RSS í‘œì¤€ í˜•ì‹
        "%a, %d %b %Y %H:%M:%S %z",  # RFC 2822
        "%Y-%m-%dT%H:%M:%S%z",  # ISO 8601
        "%Y-%m-%d %H:%M:%S",  # ì¼ë°˜ì ì¸ í˜•ì‹
        "%Y-%m-%d",  # ë‚ ì§œë§Œ
    ]

    for fmt in formats:
        try:
            return datetime.strptime(date_str.strip(), fmt)
        except ValueError:
            continue

    # í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ë„ ì‹œë„
    try:
        # "2024ë…„ 1ì›” 15ì¼" í˜•ì‹ ì²˜ë¦¬
        import re

        korean_date = re.search(r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼", date_str)
        if korean_date:
            year, month, day = korean_date.groups()
            return datetime(int(year), int(month), int(day))
    except Exception:
        pass

    return None


def _is_similar_title(title: str, seen_titles: set) -> bool:
    """
    ì œëª©ì´ ì´ë¯¸ ë³¸ ì œëª©ë“¤ê³¼ ìœ ì‚¬í•œì§€ í™•ì¸

    Args:
        title (str): í™•ì¸í•  ì œëª©
        seen_titles (set): ì´ë¯¸ ë³¸ ì œëª©ë“¤ì˜ ì§‘í•©

    Returns:
        bool: ìœ ì‚¬í•œ ì œëª©ì´ ìˆìœ¼ë©´ True
    """
    normalized_title = _normalize_title(title)

    for seen_title in seen_titles:
        # 50% ì´ìƒ ê²¹ì¹˜ë©´ ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ íŒë‹¨
        if _calculate_similarity(normalized_title, seen_title) > 0.5:
            return True

    return False


def _normalize_title(title: str) -> str:
    """
    ì œëª©ì„ ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)

    Args:
        title (str): ì›ë³¸ ì œëª©

    Returns:
        str: ì •ê·œí™”ëœ ì œëª©
    """
    import re

    # íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜
    normalized = re.sub(r"[^\w\sê°€-í£]", "", title)
    normalized = re.sub(r"\s+", " ", normalized).strip().lower()
    return normalized


def _calculate_similarity(str1: str, str2: str) -> float:
    """
    ë‘ ë¬¸ìì—´ì˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)

    Args:
        str1 (str): ì²« ë²ˆì§¸ ë¬¸ìì—´
        str2 (str): ë‘ ë²ˆì§¸ ë¬¸ìì—´

    Returns:
        float: ìœ ì‚¬ë„ (0.0 ~ 1.0)
    """
    words1 = set(str1.split())
    words2 = set(str2.split())

    if not words1 and not words2:
        return 1.0

    intersection = words1.intersection(words2)
    union = words1.union(words2)

    return len(intersection) / len(union) if union else 0.0


def _is_ad_or_promotional(title: str, description: str) -> bool:
    """
    ê´‘ê³ ë‚˜ í™ë³´ì„± ê¸°ì‚¬ì¸ì§€ í™•ì¸

    Args:
        title (str): ê¸°ì‚¬ ì œëª©
        description (str): ê¸°ì‚¬ ì„¤ëª…

    Returns:
        bool: ê´‘ê³ /í™ë³´ ê¸°ì‚¬ë©´ True
    """
    content = (title + " " + description).lower()

    # ê´‘ê³ /í™ë³´ ê´€ë ¨ í‚¤ì›Œë“œ
    ad_keywords = [
        "ê´‘ê³ ",
        "í™ë³´",
        "í˜‘ì°¬",
        "ì œíœ´",
        "í• ì¸",
        "ì´ë²¤íŠ¸",
        "í”„ë¡œëª¨ì…˜",
        "ë§ˆì¼€íŒ…",
        "ë¸Œëœë“œ",
        "ë¡ ì¹­",
        "ì˜¤í”ˆ",
        "ì‹ ì œí’ˆ",
        "ì¶œì‹œ",
        "íŠ¹ê°€",
        "ì„¸ì¼",
        "ì¿ í°",
        "í¬ì¸íŠ¸",
        "í˜œíƒ",
        "ë¬´ë£Œì²´í—˜",
        "[ê´‘ê³ ]",
        "(ê´‘ê³ )",
        "[pr]",
        "(pr)",
        "[í™ë³´]",
        "(í™ë³´)",
    ]

    return any(keyword in content for keyword in ad_keywords)


def _clean_description(description: str) -> str:
    """
    ê¸°ì‚¬ ì„¤ëª…ì—ì„œ HTML íƒœê·¸ë‚˜ ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°í•˜ê³  ì‹¤ì œ ë³¸ë¬¸ ìš”ì•½ ìƒì„±

    Args:
        description (str): ì›ë³¸ ì„¤ëª…

    Returns:
        str: ì •ì œëœ ì„¤ëª…
    """
    import re

    if not description:
        return "ìš”ì•½ ì •ë³´ ì—†ìŒ"

    # HTML íƒœê·¸ ì œê±°
    clean_desc = re.sub(r"<[^>]+>", "", description)

    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    clean_desc = re.sub(r"\s+", " ", clean_desc).strip()

    # ì¶œì²˜ ì •ë³´ë‚˜ ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì •ë³´ ì œê±°
    # ì˜ˆ: "- ë‰´ìŠ¤í•Œ", "- í™”ì„±ì €ë„" ë“±ì˜ íŒ¨í„´ ì œê±°
    clean_desc = re.sub(
        r"\s*-\s*[ê°€-í£A-Za-z]+(?:ì €ë„|ë‰´ìŠ¤|ì‹ ë¬¸|ì¼ë³´|ë°©ì†¡|ë¯¸ë””ì–´|ë‹·ì»´|\.com|\.net|\.co\.kr)?\s*$",
        "",
        clean_desc,
    )

    # URLì´ë‚˜ ë„ë©”ì¸ ì •ë³´ ì œê±°
    clean_desc = re.sub(r"https?://[^\s]+", "", clean_desc)
    clean_desc = re.sub(r"www\.[^\s]+", "", clean_desc)
    clean_desc = re.sub(r"[a-zA-Z0-9.-]+\.(com|net|co\.kr|org)\b", "", clean_desc)

    # v.daum.net ê°™ì€ íŒ¨í„´ ì œê±°
    clean_desc = re.sub(r"v\.[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", "", clean_desc)

    # ê¸°ì‚¬ ì œëª©ê³¼ ë™ì¼í•œ ë¶€ë¶„ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    # ë§Œì•½ ì„¤ëª…ì´ ì œëª©ê³¼ ê±°ì˜ ë™ì¼í•˜ë‹¤ë©´ ë” ì˜ë¯¸ìˆëŠ” ìš”ì•½ ìƒì„±
    if len(clean_desc.strip()) < 20:
        return "ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”"

    # ì—°ì†ëœ ê³µë°± ë‹¤ì‹œ ì •ë¦¬
    clean_desc = re.sub(r"\s+", " ", clean_desc).strip()

    # ë„ˆë¬´ ê¸´ ì„¤ëª…ì€ ì˜ë¼ë‚´ê¸° (150ì ì œí•œìœ¼ë¡œ ì¶•ì†Œ)
    if len(clean_desc) > 150:
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸° ì‹œë„
        sentences = clean_desc.split(". ")
        if len(sentences) > 1:
            clean_desc = sentences[0] + "."
            if len(clean_desc) > 150:
                clean_desc = clean_desc[:147] + "..."
        else:
            clean_desc = clean_desc[:147] + "..."

    return clean_desc if clean_desc.strip() else "ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”"


def _extract_source(source_info: Any) -> str:
    """
    ë‰´ìŠ¤ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ

    Args:
        source_info: ì¶œì²˜ ì •ë³´ (dict ë˜ëŠ” str)

    Returns:
        str: ì¶œì²˜ ì´ë¦„
    """
    if isinstance(source_info, dict):
        return source_info.get("title", source_info.get("name", "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"))
    elif isinstance(source_info, str):
        return source_info
    else:
        return "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"


def _extract_source_from_title_and_description(
    title: str, description: str, link: str = ""
) -> str:
    """
    ì œëª©, ì„¤ëª…, ë§í¬ì—ì„œ ë‰´ìŠ¤ ì¶œì²˜ë¥¼ ì¶”ì¶œ

    Args:
        title (str): ê¸°ì‚¬ ì œëª©
        description (str): ê¸°ì‚¬ ì„¤ëª…
        link (str): ê¸°ì‚¬ ë§í¬

    Returns:
        str: ì¶œì²˜ ì´ë¦„
    """
    import re

    # 1. ì œëª©ì—ì„œ ì¶œì²˜ ì¶”ì¶œ (ì œëª© ëì— "- ì¶œì²˜ëª…" íŒ¨í„´)
    title_source_match = re.search(
        r"\s*-\s*([ê°€-í£A-Za-z0-9]+(?:ì €ë„|ë‰´ìŠ¤|ì‹ ë¬¸|ì¼ë³´|ë°©ì†¡|ë¯¸ë””ì–´|íƒ€ì„ì¦ˆ|í—¤ëŸ´ë“œ|í¬ìŠ¤íŠ¸|íˆ¬ë°ì´|ë°ì¼ë¦¬|ìœ„í´ë¦¬))\s*$",
        title,
    )
    if title_source_match:
        return title_source_match.group(1)

    # 2. ì„¤ëª…ì—ì„œ ì¶œì²˜ ì¶”ì¶œ
    desc_source_match = re.search(
        r"\s*-?\s*([ê°€-í£A-Za-z0-9]+(?:ì €ë„|ë‰´ìŠ¤|ì‹ ë¬¸|ì¼ë³´|ë°©ì†¡|ë¯¸ë””ì–´|íƒ€ì„ì¦ˆ|í—¤ëŸ´ë“œ|í¬ìŠ¤íŠ¸|íˆ¬ë°ì´|ë°ì¼ë¦¬|ìœ„í´ë¦¬))\s*$",
        description,
    )
    if desc_source_match:
        return desc_source_match.group(1)

    # 3. ë§í¬ì—ì„œ ë„ë©”ì¸ ê¸°ë°˜ ì¶œì²˜ ì¶”ì¶œ
    if link:
        try:
            from urllib.parse import urlparse

            parsed_url = urlparse(link)
            domain = parsed_url.netloc.lower()

            # ì£¼ìš” ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ë§¤í•‘
            domain_mapping = {
                "news.naver.com": "ë„¤ì´ë²„ë‰´ìŠ¤",
                "news.daum.net": "ë‹¤ìŒë‰´ìŠ¤",
                "v.daum.net": "ë‹¤ìŒë‰´ìŠ¤",
                "news.google.com": "êµ¬ê¸€ë‰´ìŠ¤",
                "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
                "yonhapnews.co.kr": "ì—°í•©ë‰´ìŠ¤",
                "chosun.com": "ì¡°ì„ ì¼ë³´",
                "donga.com": "ë™ì•„ì¼ë³´",
                "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
                "hani.co.kr": "í•œê²¨ë ˆ",
                "khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
                "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
                "mk.co.kr": "ë§¤ì¼ê²½ì œ",
                "hankyung.com": "í•œêµ­ê²½ì œ",
                "sbs.co.kr": "SBS",
                "kbs.co.kr": "KBS",
                "mbc.co.kr": "MBC",
                "newspim.com": "ë‰´ìŠ¤í•Œ",
                "news1.kr": "ë‰´ìŠ¤1",
                "pressian.com": "í”„ë ˆì‹œì•ˆ",
                "ohmynews.com": "ì˜¤ë§ˆì´ë‰´ìŠ¤",
                "sisain.co.kr": "ì‹œì‚¬IN",
                "hankookilbo.com": "í•œêµ­ì¼ë³´",
                "seoul.co.kr": "ì„œìš¸ì‹ ë¬¸",
                "munhwa.com": "ë¬¸í™”ì¼ë³´",
                "dt.co.kr": "ë””ì§€í„¸íƒ€ì„ìŠ¤",
                "etnews.com": "ì „ìì‹ ë¬¸",
                "zdnet.co.kr": "ZDNet Korea",
            }

            # ì •í™•í•œ ë„ë©”ì¸ ë§¤ì¹­
            if domain in domain_mapping:
                return domain_mapping[domain]

            # ë¶€ë¶„ ë„ë©”ì¸ ë§¤ì¹­
            for known_domain, source_name in domain_mapping.items():
                if known_domain in domain or domain in known_domain:
                    return source_name

            # ë„ë©”ì¸ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ ì‹œë„
            domain_parts = domain.replace("www.", "").split(".")
            if len(domain_parts) >= 2:
                main_domain = domain_parts[0]
                # ì–¸ë¡ ì‚¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
                if any(
                    keyword in main_domain
                    for keyword in [
                        "news",
                        "journal",
                        "daily",
                        "times",
                        "post",
                        "herald",
                    ]
                ):
                    return main_domain.title()
                # í•œêµ­ ì–¸ë¡ ì‚¬ ë„ë©”ì¸ íŒ¨í„´
                if main_domain.endswith(("news", "journal")):
                    return (
                        main_domain.replace("news", "ë‰´ìŠ¤")
                        .replace("journal", "ì €ë„")
                        .title()
                    )

        except Exception:
            pass

    # 4. ê¸°íƒ€ íŒ¨í„´ìœ¼ë¡œ ì¶œì²˜ ì¶”ì¶œ
    combined_text = f"{title} {description}"

    # ê´„í˜¸ ì•ˆì˜ ì¶œì²˜ ì •ë³´
    bracket_source = re.search(
        r"\(([ê°€-í£A-Za-z0-9]+(?:ì €ë„|ë‰´ìŠ¤|ì‹ ë¬¸|ì¼ë³´|ë°©ì†¡|ë¯¸ë””ì–´))\)", combined_text
    )
    if bracket_source:
        return bracket_source.group(1)

    # ëŒ€ê´„í˜¸ ì•ˆì˜ ì¶œì²˜ ì •ë³´
    square_bracket_source = re.search(
        r"\[([ê°€-í£A-Za-z0-9]+(?:ì €ë„|ë‰´ìŠ¤|ì‹ ë¬¸|ì¼ë³´|ë°©ì†¡|ë¯¸ë””ì–´))\]", combined_text
    )
    if square_bracket_source:
        return square_bracket_source.group(1)

    return "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"


def _get_real_url_from_google_news(google_news_url: str) -> str:
    """
    Google News URLì—ì„œ ì‹¤ì œ ì›ë³¸ ê¸°ì‚¬ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        google_news_url (str): Google News RSS URL

    Returns:
        str: ì‹¤ì œ ê¸°ì‚¬ URL ë˜ëŠ” ì›ë³¸ URL
    """
    try:
        if not google_news_url or "news.google.com" not in google_news_url:
            return google_news_url

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        # Google News ë§í¬ë¥¼ ë”°ë¼ê°€ì„œ ì‹¤ì œ URL ì–»ê¸°
        response = requests.get(
            google_news_url, headers=headers, timeout=10, allow_redirects=True
        )

        # ìµœì¢… ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ URL ë°˜í™˜
        final_url = response.url

        # Google News ë‚´ë¶€ URLì´ë©´ í¬ê¸°
        if "news.google.com" in final_url:
            return google_news_url

        return final_url

    except Exception:
        return google_news_url


def _fetch_article_content(url: str) -> str:
    """
    ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        url (str): ê¸°ì‚¬ URL

    Returns:
        str: ì¶”ì¶œëœ ë³¸ë¬¸ ë‚´ìš©
    """
    try:
        # Google News URLì¸ ê²½ìš° ì‹¤ì œ URLë¡œ ë³€í™˜ ì‹œë„
        real_url = _get_real_url_from_google_news(url)

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        response = requests.get(real_url, headers=headers, timeout=10)
        if response.status_code != 200:
            return ""

        soup = BeautifulSoup(response.content, "html.parser")

        # ê´‘ê³ , ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
        for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
            script.decompose()

        # ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì˜ ë³¸ë¬¸ ì„ íƒì ì‹œë„
        content_selectors = [
            # ë„¤ì´ë²„ ë‰´ìŠ¤
            "#dic_area",
            ".go_trans._article_content",
            # ë‹¤ìŒ ë‰´ìŠ¤
            ".news_view .article_view",
            ".news_view .view_content",
            # ì¼ë°˜ì ì¸ ì„ íƒì
            "article",
            ".article-content",
            ".article_content",
            ".news-content",
            ".content",
            ".view_content",
            ".article-body",
            ".entry-content",
            'div[class*="content"]',
            'div[class*="article"]',
            # ì§€ì—­ ì–¸ë¡ ì‚¬ ì„ íƒì ì¶”ê°€
            ".article_txt",
            ".news_article",
            ".view_text",
            # ë§ˆì§€ë§‰ ì‹œë„: p íƒœê·¸ë“¤
            "p",
        ]

        content = ""
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                if selector == "p":
                    # p íƒœê·¸ì˜ ê²½ìš° ì—¬ëŸ¬ ê°œë¥¼ í•©ì¹¨ (ë” ë§ì€ ë¬¸ë‹¨ í¬í•¨)
                    paragraphs = []
                    for elem in elements[:10]:  # ì²˜ìŒ 10ê°œ ë¬¸ë‹¨ê¹Œì§€
                        text = elem.get_text().strip()
                        if (
                            len(text) > 20  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´
                            and not text.startswith(
                                (
                                    "ê¸°ì",
                                    "ì‚¬ì§„",
                                    "ì˜ìƒ",
                                    "ì¶œì²˜",
                                    "ì €ì‘ê¶Œ",
                                    "â–²",
                                    "â– ",
                                    "â€»",
                                )
                            )
                            and not text.endswith(("ê¸°ì", "ì œê³µ"))
                            and "ê´‘ê³ " not in text
                            and "í™ë³´" not in text
                        ):
                            paragraphs.append(text)
                    content = " ".join(paragraphs)
                else:
                    content = elements[0].get_text().strip()

                if len(content) > 100:  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                    break

        # í…ìŠ¤íŠ¸ ì •ì œ
        if content:
            import re

            # ì—°ì†ëœ ê³µë°±, ì¤„ë°”ê¿ˆ ì •ë¦¬
            content = re.sub(r"\s+", " ", content).strip()
            # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
            content = re.sub(r"(ê¸°ì\s*=?\s*[ê°€-í£]+|[ê°€-í£]+\s*ê¸°ì)", "", content)
            content = re.sub(r"(ì‚¬ì§„\s*=?\s*[^.]*|ì˜ìƒ\s*=?\s*[^.]*)", "", content)
            content = re.sub(r"(â–²|â– |â€»)[^.]*", "", content)

            # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ì²˜ìŒ 500ìë§Œ ì‚¬ìš©
            if len(content) > 500:
                content = content[:500]

        return content

    except Exception:
        return ""


def _generate_summary(title: str, content: str, description: str = "") -> str:
    """
    ê¸°ì‚¬ ì œëª©, ë³¸ë¬¸, ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        title (str): ê¸°ì‚¬ ì œëª©
        content (str): ê¸°ì‚¬ ë³¸ë¬¸
        description (str): ê¸°ì‚¬ ì„¤ëª…

    Returns:
        str: ìƒì„±ëœ ìš”ì•½
    """
    import re

    # ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ë³¸ë¬¸ ê¸°ë°˜ ìš”ì•½ ìƒì„±
    if content and len(content.strip()) > 50:
        # ë³¸ë¬¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        sentences = re.split(r"[.!?]", content)
        meaningful_sentences = []

        # ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ìš”ì•½ í’ˆì§ˆ í–¥ìƒìš©)
        title_keywords = set(re.findall(r"[ê°€-í£]{2,}", title))

        for sentence in sentences:
            sentence = sentence.strip()

            # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ í•„í„°ë§
            if (
                len(sentence) > 20
                and len(sentence) < 200  # ì ì ˆí•œ ê¸¸ì´
                and not sentence.startswith(
                    ("ê¸°ì", "ì‚¬ì§„", "ì˜ìƒ", "ì¶œì²˜", "ì €ì‘ê¶Œ", "â–²", "â– ", "â€»", "ì´ë©”ì¼")
                )
                and not sentence.endswith(("ê¸°ì", "ì œê³µ", "=ì—°í•©ë‰´ìŠ¤"))
                and not re.match(r"^[\d\s\-()]+$", sentence)  # ìˆ«ìë§Œ ìˆëŠ” ë¬¸ì¥ ì œì™¸
                and "ê´‘ê³ " not in sentence
                and "í™ë³´" not in sentence
                and "copyright" not in sentence.lower()
            ):
                # ì œëª©ê³¼ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì¥ ìš°ì„  ì„ íƒ
                sentence_keywords = set(re.findall(r"[ê°€-í£]{2,}", sentence))
                relevance_score = len(title_keywords.intersection(sentence_keywords))

                meaningful_sentences.append((sentence, relevance_score))

        if meaningful_sentences:
            # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
            meaningful_sentences.sort(key=lambda x: x[1], reverse=True)

            # ìƒìœ„ 2ê°œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ êµ¬ì„±
            selected_sentences = [sent[0] for sent in meaningful_sentences[:2]]
            summary = ". ".join(selected_sentences)

            # ìš”ì•½ì´ ì œëª©ê³¼ ë„ˆë¬´ ìœ ì‚¬í•˜ë©´ ë‹¤ë¥¸ ë¬¸ì¥ ì‹œë„
            if (
                _calculate_similarity(
                    _normalize_title(title), _normalize_title(summary)
                )
                > 0.7
            ):
                if len(meaningful_sentences) > 2:
                    selected_sentences = [sent[0] for sent in meaningful_sentences[1:3]]
                    summary = ". ".join(selected_sentences)

            if len(summary) > 150:
                summary = summary[:147] + "..."
            elif not summary.endswith("."):
                summary += "."

            return summary

    # ë³¸ë¬¸ì´ ì—†ê±°ë‚˜ ì§§ìœ¼ë©´ ì„¤ëª…ì—ì„œ ìš”ì•½ ì¶”ì¶œ ì‹œë„
    if description:
        # ì„¤ëª…ì—ì„œ ì œëª©ê³¼ ë‹¤ë¥¸ ë¶€ë¶„ ì°¾ê¸°
        cleaned_desc = _clean_description(description)
        if (
            cleaned_desc
            and cleaned_desc
            not in ["ìš”ì•½ ì •ë³´ ì—†ìŒ", "ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”"]
            and _calculate_similarity(
                _normalize_title(title), _normalize_title(cleaned_desc)
            )
            < 0.8
        ):
            return cleaned_desc

    # ì œëª©ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œí•´ì„œ ê¸°ë³¸ ìš”ì•½ ìƒì„±
    import re

    # ë‚ ì§œ, ì¥ì†Œ, í–‰ì‚¬ëª… ë“± ì¶”ì¶œ
    title_without_source = re.sub(
        r"\s*-\s*[ê°€-í£A-Za-z0-9]+(?:ì €ë„|ë‰´ìŠ¤|ì‹ ë¬¸|ì¼ë³´|ë°©ì†¡|ë¯¸ë””ì–´)?\s*$", "", title
    )

    if "ê°œìµœ" in title_without_source or "ì§„í–‰" in title_without_source:
        return f"{title_without_source.strip()}ì— ëŒ€í•œ ìƒì„¸ ë‚´ìš©ì„ ê¸°ì‚¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    elif "ë°œí‘œ" in title_without_source or "ê³„íš" in title_without_source:
        return (
            f"{title_without_source.strip()}ì— ê´€í•œ êµ¬ì²´ì ì¸ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        )
    else:
        return "ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”."


def format_news_info_text(news_list: List[Dict[str, Any]], keyword: str) -> str:
    """
    ë‰´ìŠ¤ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.

    Args:
        news_list (List[Dict[str, Any]]): ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡
        keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ

    Returns:
        str: í¬ë§·íŒ…ëœ ë‰´ìŠ¤ ì •ë³´ í…ìŠ¤íŠ¸
    """
    if not news_list:
        return f"""
"{keyword}" ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤:
- ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.
"""

    result = f"""
"{keyword}" ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ({len(news_list)}ê±´):

"""

    for i, news in enumerate(news_list, 1):
        result += f"{i}. {news.get('title', 'ì œëª© ì—†ìŒ')}\n"
        result += f"   ìš”ì•½: {news.get('summary', 'ìš”ì•½ ì—†ìŒ')}\n"
        result += f"   ì¶œì²˜: {news.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
        result += f"   ë“±ë¡: {news.get('pub_date', 'ì‹œê°„ ì •ë³´ ì—†ìŒ')}\n"

        if news.get("link"):
            result += f"   ë§í¬: {news['link']}\n"

        result += "\n"

    return result


def format_news_info_html(news_list: List[Dict[str, Any]], keyword: str) -> str:
    """
    ë‰´ìŠ¤ ì •ë³´ë¥¼ HTML í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.

    Args:
        news_list (List[Dict[str, Any]]): ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡
        keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ

    Returns:
        str: í¬ë§·íŒ…ëœ ë‰´ìŠ¤ ì •ë³´ HTML
    """
    if not news_list:
        return f"""
<h3>ğŸ“° "{keyword}" ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤</h3>
<p><em>ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.</em></p>
"""

    result = f"""
<h3>ğŸ“° "{keyword}" ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ({len(news_list)}ê±´)</h3>
<div style="margin-left: 10px;">
"""

    for i, news in enumerate(news_list, 1):
        title = news.get("title", "ì œëª© ì—†ìŒ")
        summary = news.get("summary", "ìš”ì•½ ì—†ìŒ")
        source = news.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
        pub_date = news.get("pub_date", "ì‹œê°„ ì •ë³´ ì—†ìŒ")
        link = news.get("link", "")

        result += f"""
<div style="margin-bottom: 15px; padding: 10px; border-left: 3px solid #007acc;">
    <h4 style="margin: 0 0 5px 0; color: #007acc;">
        {i}. {title}
    </h4>
    <p style="margin: 5px 0; color: #555; font-size: 14px;">
        ğŸ“ <strong>ìš”ì•½:</strong> {summary}
    </p>
    <p style="margin: 5px 0; color: #777; font-size: 12px;">
        ğŸ“º <strong>ì¶œì²˜:</strong> {source} |
        ğŸ•’ <strong>ë“±ë¡:</strong> {pub_date}
    </p>
"""

        if link:
            result += f"""
    <p style="margin: 5px 0; font-size: 12px;">
        ğŸ”— <a href="{link}" style="color: #007acc;">ê¸°ì‚¬ ë§í¬</a>
    </p>
"""

        result += "</div>\n"

    result += "</div>\n"

    return result
